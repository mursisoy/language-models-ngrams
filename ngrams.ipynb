{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXh403_O-BeR"
   },
   "source": [
    "# Sistemas Inteligentes práctica 1\n",
    "## Generación de texto predictivo basado en N-grams\n",
    "### 2020-2021\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TYkfT7F4-BeW"
   },
   "outputs": [],
   "source": [
    "# Práctica basada en:\n",
    "# https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/\n",
    "# https://nlpforhackers.io/language-models/\n",
    "    \n",
    "# Más información en:\n",
    "# https://www.nltk.org/api/nltk.html\n",
    "# https://web.stanford.edu/~jurafsky/slp3/ (--> Language Modeling with N-Grams)\n",
    "\n",
    "# Libros a descargar en texto plano UTF-8:\n",
    "# https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AAmOTjdo-BeY"
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, ngrams\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8S-eXQ5-BeY"
   },
   "source": [
    "## Trigrams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9Kjl56v5-BeZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None hola\n",
      "None hola esto\n",
      "hola esto es\n",
      "esto es un\n",
      "es un ejemplo\n",
      "un ejemplo que\n",
      "ejemplo que lo\n",
      "que lo escribo\n",
      "lo escribo yo\n",
      "escribo yo mismo\n",
      "yo mismo .\n",
      "mismo . hola\n",
      ". hola esto\n",
      "hola esto puede\n",
      "esto puede ser\n",
      "puede ser otro\n",
      "ser otro ejemplo\n",
      "otro ejemplo o\n",
      "ejemplo o es\n",
      "o es un\n",
      "es un coche\n",
      "un coche o\n",
      "coche o es\n",
      "o es rojo\n",
      "es rojo .\n",
      "rojo . hola\n",
      ". hola esto\n",
      "hola esto debe\n",
      "esto debe ser\n",
      "debe ser un\n",
      "ser un ejemplo\n",
      "un ejemplo .\n",
      "ejemplo . Aunque\n",
      ". Aunque no\n",
      "Aunque no lo\n",
      "no lo tengo\n",
      "lo tengo tan\n",
      "tengo tan claro\n",
      "tan claro ,\n",
      "claro , sabes\n",
      ", sabes ?\n",
      "sabes ? Qué\n",
      "? Qué te\n",
      "Qué te parece\n",
      "te parece el\n",
      "parece el resultado\n",
      "el resultado ?\n",
      "resultado ? None\n",
      "? None None\n"
     ]
    }
   ],
   "source": [
    "# Create a placeholder for model\n",
    "modelTrigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "log=1\n",
    "\n",
    "#Get the text\n",
    "\n",
    "with open(\"pg2000.txt\", \"r\", encoding=\"utf-8\") as myfile:\n",
    "    texto = myfile.read()\n",
    "\n",
    "texto='hola esto es un ejemplo que lo escribo yo mismo. hola esto puede ser otro ejemplo o es un coche o es rojo. hola esto debe ser un ejemplo. Aunque no lo tengo tan claro, sabes? Qué te parece el resultado?'    \n",
    "\n",
    "#Sist Intel práctica 1: Introducimos el texto primero en un string con el fin de comprender cómo funcionan,\n",
    "#para posteriormente poder trabajar con ficheros de texto\n",
    "\n",
    "#Prepare the text\n",
    "texto=texto[:] #consider to delete the head and what you consider, goal is to get the pure text from the \"raw\" text\n",
    "texto=texto.replace(\".\",\" . \")\n",
    "texto=texto.replace(\",\",\" , \")\n",
    "texto=texto.replace(\"?\",\" ? \")\n",
    "texto=texto.replace(\"¿\",\" ¿ \")\n",
    "texto=texto.replace(\"!\",\" ! \")\n",
    "texto=texto.replace(\"¡\",\" ¡ \")\n",
    "#Sist Intel práctica 1: ¿Por qué es necesaria la preparación del texto?\n",
    "\n",
    "#Get the text as list of words\n",
    "texto2=texto.split()\n",
    "\n",
    "# Build the model based on Trigrams\n",
    "for w1, w2, w3 in trigrams(texto2, pad_right=True, pad_left=True):\n",
    "        if log:\n",
    "            print(w1, w2, w3)\n",
    "        modelTrigrams[(w1, w2)][w3] += 1\n",
    "        \n",
    "# Let's transform the counts to probabilities\n",
    "for w1_w2 in modelTrigrams:\n",
    "    total_count = float(sum(modelTrigrams[w1_w2].values()))\n",
    "    for w3 in modelTrigrams[w1_w2]:\n",
    "        modelTrigrams[w1_w2][w3] /= total_count     \n",
    "        \n",
    "#Sist Intel práctica 1: ¿Puedes explicar qué y con qué finalidad se hace el código de \n",
    "# la construcción del modelo y en su transformación a probabilidades? ¿Por qué aparecen None en los trigrams? ten\n",
    "#en cuenta los parámetros: trigrams(texto2, pad_right=True, pad_left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3Rot_mk8-Beb",
    "outputId": "674eb064-2470-44ea-d2de-96bb91f0a3c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ejemplo': 0.5, 'coche': 0.5}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the next word\n",
    "dict(modelTrigrams[\"es\", \"un\"])\n",
    "\n",
    "#Sist Intel práctica 1: Realiza diferentes pruebas con secuencias de dos palabras que se encuentren seguidas, ¿tiene sentido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QhJDkklN-Beb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None :  {'hola': 1.0}\n",
      "None hola :  {'esto': 1.0}\n",
      "hola esto :  {'es': 0.3333333333333333, 'puede': 0.3333333333333333, 'debe': 0.3333333333333333}\n",
      "esto es :  {'un': 1.0}\n",
      "es un :  {'ejemplo': 0.5, 'coche': 0.5}\n",
      "un ejemplo :  {'que': 0.5, '.': 0.5}\n",
      "ejemplo que :  {'lo': 1.0}\n",
      "que lo :  {'escribo': 1.0}\n",
      "lo escribo :  {'yo': 1.0}\n",
      "escribo yo :  {'mismo': 1.0}\n",
      "yo mismo :  {'.': 1.0}\n",
      "mismo . :  {'hola': 1.0}\n",
      ". hola :  {'esto': 1.0}\n",
      "esto puede :  {'ser': 1.0}\n",
      "puede ser :  {'otro': 1.0}\n",
      "ser otro :  {'ejemplo': 1.0}\n",
      "otro ejemplo :  {'o': 1.0}\n",
      "ejemplo o :  {'es': 1.0}\n",
      "o es :  {'un': 0.5, 'rojo': 0.5}\n",
      "un coche :  {'o': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# List trigrams with calculated probabilities\n",
    "numIter = 0\n",
    "numIterMax = 20\n",
    "for x1,x2 in modelTrigrams:\n",
    "    print(x1,x2, ': ', dict(modelTrigrams[x1, x2]))\n",
    "    numIter+=1\n",
    "    if numIter >= numIterMax:\n",
    "        break\n",
    "        \n",
    " #Sist Intel práctica 1: ¿Qué significa el resultado obtenido? ¿Por qué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YhFt5T4P-Bec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umbral random:  0.9227224338738217\n",
      "palabras anteriores --> ('hola', 'esto')\n",
      "palabra:  es , probabilidad de palabra:  0.3333333333333333  --> probabilidad acumulada:  0.3333333333333333\n",
      "palabras anteriores --> ('hola', 'esto')\n",
      "palabra:  puede , probabilidad de palabra:  0.3333333333333333  --> probabilidad acumulada:  0.6666666666666666\n",
      "palabras anteriores --> ('hola', 'esto')\n",
      "palabra:  debe , probabilidad de palabra:  0.3333333333333333  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> debe\n",
      "frase construida:  hola esto debe\n",
      "umbral random:  0.23433140397375463\n",
      "palabras anteriores --> ('esto', 'debe')\n",
      "palabra:  ser , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> ser\n",
      "frase construida:  hola esto debe ser\n",
      "umbral random:  0.3422969218498324\n",
      "palabras anteriores --> ('debe', 'ser')\n",
      "palabra:  un , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> un\n",
      "frase construida:  hola esto debe ser un\n",
      "umbral random:  0.8459737528979818\n",
      "palabras anteriores --> ('ser', 'un')\n",
      "palabra:  ejemplo , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> ejemplo\n",
      "frase construida:  hola esto debe ser un ejemplo\n",
      "umbral random:  0.1297984835726429\n",
      "palabras anteriores --> ('un', 'ejemplo')\n",
      "palabra:  que , probabilidad de palabra:  0.5  --> probabilidad acumulada:  0.5\n",
      "\tpalabra buena: ==> que\n",
      "frase construida:  hola esto debe ser un ejemplo que\n",
      "umbral random:  0.1322579388212165\n",
      "palabras anteriores --> ('ejemplo', 'que')\n",
      "palabra:  lo , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> lo\n",
      "frase construida:  hola esto debe ser un ejemplo que lo\n",
      "umbral random:  0.39313411823662103\n",
      "palabras anteriores --> ('que', 'lo')\n",
      "palabra:  escribo , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> escribo\n",
      "frase construida:  hola esto debe ser un ejemplo que lo escribo\n",
      "umbral random:  0.21208705411909246\n",
      "palabras anteriores --> ('lo', 'escribo')\n",
      "palabra:  yo , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> yo\n",
      "frase construida:  hola esto debe ser un ejemplo que lo escribo yo\n",
      "umbral random:  0.015319019528548083\n",
      "palabras anteriores --> ('escribo', 'yo')\n",
      "palabra:  mismo , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> mismo\n",
      "frase construida:  hola esto debe ser un ejemplo que lo escribo yo mismo\n",
      "umbral random:  0.6292750979395403\n",
      "palabras anteriores --> ('yo', 'mismo')\n",
      "palabra:  . , probabilidad de palabra:  1.0  --> probabilidad acumulada:  1.0\n",
      "\tpalabra buena: ==> .\n",
      "frase construida:  hola esto debe ser un ejemplo que lo escribo yo mismo .\n",
      "hola esto debe ser un ejemplo que lo escribo yo mismo .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# starting words\n",
    "palabra1 = 'hola'\n",
    "palabra2 = 'esto'\n",
    "text = [palabra1, palabra2]\n",
    "\n",
    "errorNoAvailable = 0\n",
    "\n",
    "if dict(modelTrigrams[text[0], text[1]])=={}:\n",
    "    print('error: secuencia no encontrada')\n",
    "    errorNoAvailable = 1\n",
    "\n",
    "sentence_finished = False\n",
    "numberOfwords = 0\n",
    "maxNumberofWords = 10\n",
    " \n",
    "while (not sentence_finished) and errorNoAvailable==0:\n",
    "    # select a random probability threshold  \n",
    "    r = random.random()\n",
    "    #log:\n",
    "    if log:\n",
    "        print('umbral random: ', r)\n",
    "    accumulator = .0\n",
    "    \n",
    "    for word in modelTrigrams[tuple(text[-2:])].keys():\n",
    "        accumulator += modelTrigrams[tuple(text[-2:])][word]\n",
    "        #log:\n",
    "        if log:\n",
    "            print('palabras anteriores -->', tuple(text[-2:]))\n",
    "            print('palabra: ', word, ', probabilidad de palabra: ', modelTrigrams[tuple(text[-2:])][word],' --> probabilidad acumulada: ', accumulator)        \n",
    "                      \n",
    "        # select words that are above the probability threshold\n",
    "        if accumulator >= r:\n",
    "            numberOfwords+=1\n",
    "            text.append(word)\n",
    "            #log:\n",
    "            if log:\n",
    "                print('\\tpalabra buena: ==>', word)\n",
    "                print('frase construida: ', ' '.join([t for t in text if t]))\n",
    "            break\n",
    "    if text[-2:] == [None, None]:\n",
    "        sentence_finished = True\n",
    "        \n",
    "    if numberOfwords>=maxNumberofWords:\n",
    "        sentence_finished = True\n",
    " \n",
    "if errorNoAvailable==0:\n",
    "    print (' '.join([t for t in text if t]))\n",
    "\n",
    "#Sist Intel práctica 1: Explicar cómo funciona la generación de frases, desde la variable r, accumulator, puedes utilizar el log obtenido\n",
    "# para interepretar el algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulwoPcnZ-Bee"
   },
   "source": [
    "## Bigrams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "F75IrkDQ-Bef"
   },
   "outputs": [],
   "source": [
    "#Sist Intel práctica 1: Extiende el model de trigrams a bigrams, puedes copiar las mismas celdas\n",
    "# del modelo basado en trigrams y adaptarlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QTNCY2OL-Bef"
   },
   "outputs": [],
   "source": [
    "#Sist Intel práctica 1: Puedes comparar y extraer conclusiones de las frases generadas en trigrams y bigrams?\n",
    "#Sist Intel práctica 1: Puedes utilizar diferentes fuentes de datos (libros) y extraer conclusiones\n",
    "# de las frases generadas en trigrams y bigrams en función de as fuentes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hcfrTArl3mN"
   },
   "source": [
    "# Ngrams model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIElhsEsOQdx",
    "outputId": "5d29eaab-b39e-4e22-841f-b234c78f6acb"
   },
   "outputs": [],
   "source": [
    "%pip install gutenberg-cleaner\n",
    "from gutenberg_cleaner import simple_cleaner, super_cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tEQId8j7-Bef"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "from operator import methodcaller\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk import ngrams\n",
    "from collections import defaultdict\n",
    "import pydot\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(model, filename):\n",
    "    if len(model) > 50:\n",
    "        print(\"Model is too big\")\n",
    "    else:\n",
    "        # Draw the graph\n",
    "        graphnodes = []\n",
    "        written = []\n",
    "        def traverse(w):\n",
    "            for nw in model[w]:\n",
    "                if (w[-1],nw) not in written:\n",
    "                    graphnodes.append(f\"\\\"{w[-1]}\\\" [label=\\\"{w[-1]}\\\"];\")\n",
    "                    graphnodes.append(f\"\\\"{nw}\\\" [label=\\\"{nw}\\\"];\")\n",
    "                    path_label = f\"({w[0:-1]})\" if len(w[0:-1]) else \"\"\n",
    "                    graphnodes.append(f\"\\\"{w[-1]}\\\" -> \\\"{nw}\\\" [label=\\\"{path_label}{model[w][nw]:.2f}\\\"];\")\n",
    "                    written.append((w[-1],nw))\n",
    "                    traverse((w[1:])+(nw,))\n",
    "\n",
    "        for w in model:\n",
    "            traverse(w)\n",
    "\n",
    "        dot_string = \"digraph G{\\nrankdir=LR\\n\" + \"\\n\".join(graphnodes) + \"}\"\n",
    "        graph = pydot.graph_from_dot_data(dot_string)\n",
    "        with open(f\"./{filename}.pdf\", 'wb') as f:\n",
    "            f.write(graph[0].create_pdf())\n",
    "        graph[0].write_png(f\"./{filename}.png\")\n",
    "        graph_png = cv2.imread(f\"./{filename}.png\",1)\n",
    "        height, width, channels = graph_png.shape\n",
    "        plt.figure(figsize = (width/96,height/96),dpi=96)\n",
    "        plt.imshow(graph_png)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C34c1Jt1OTvI",
    "outputId": "58e6662f-ba7f-49a1-954b-1289e5fb8aa2"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !ln -s /content/drive/MyDrive/sistint_p1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rqN8kKvWO5Qt"
   },
   "outputs": [],
   "source": [
    "#modify according to your base path\n",
    "# base_path=\"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wWBoQpxZOess"
   },
   "outputs": [],
   "source": [
    "# Split sentences by dot\n",
    "def split_sentences(text):\n",
    "  # From: https://stackoverflow.com/a/25736082\n",
    "  # https://regex101.com/r/nG1gU7/27\n",
    "  return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s',text)\n",
    "\n",
    "# Clean input text, spaces between symbols\n",
    "def text_cleaner(text):\n",
    "  clean_text = text\n",
    "  clean_text = clean_text.replace(\"\\'\",\"\")\n",
    "  clean_text = clean_text.replace(\"\\\"\",\"\")\n",
    "  clean_text = clean_text.replace(\"—\",\"\")\n",
    "  clean_text = clean_text.replace(\"♫\",\"\")\n",
    "  clean_text = clean_text.replace(\"#\",\"\")\n",
    "  clean_text = clean_text.replace(\"♪\",\"\")\n",
    "  clean_text = clean_text.replace(\"«\",\"\")\n",
    "  clean_text = clean_text.replace(\"»\",\"\")\n",
    "  clean_text = re.sub(r\"\\([^\\)]+\\)\",'',clean_text)\n",
    "  clean_text = re.sub(r\"([¿¡!\\$%&\\*\\+,-\\./:;<=>\\?@\\\\\\^_`{\\|}~])\",r\" \\1 \",clean_text)\n",
    "  return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YodghOWOT3pA"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train ngram model\n",
    "def train_ngram_model(words, n=3):\n",
    "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "  # Build the model based on ngrams\n",
    "  for w in ngrams(words, n, pad_right=True, pad_left=True):\n",
    "    model[w[0:-1]][w[-1]] += 1\n",
    "\n",
    "  # Let's transform the counts to probabilities\n",
    "  for w in model:\n",
    "    total_count = float(sum(model[w].values()))\n",
    "    for nw in model[w]:\n",
    "      model[w][nw] /= total_count  \n",
    "  return model\n",
    "\n",
    "# Generate sentences (dot ended) with ngram model\n",
    "def generate_sentences(begins_with, ngram_model, number_of_sentences=3):\n",
    "    \n",
    "  errorNoAvailable = 0\n",
    "  \n",
    "  # Obtain ngram grade\n",
    "  ngram_model_n = len(list(ngram_model.keys())[0])\n",
    "\n",
    "  text = begins_with\n",
    "\n",
    "  # Look for a match\n",
    "  if dict(ngram_model[tuple(text[-(ngram_model_n):])])=={}:\n",
    "      print('error: secuencia no encontrada')\n",
    "      return []\n",
    "\n",
    "  # Sentence counter\n",
    "  sentences_generated = 0\n",
    "\n",
    "  # Loop til number of sentences\n",
    "  while sentences_generated < number_of_sentences:\n",
    "    # Generate a random number\n",
    "    r = random.random()\n",
    "\n",
    "    # Accumulated probability\n",
    "    accumulator = .0\n",
    "  \n",
    "    for word in ngram_model[tuple(text[-(ngram_model_n):])].keys():\n",
    "      accumulator += ngram_model[tuple(text[-(ngram_model_n):])][word]  \n",
    "      # Select words that are above the random probability threshold\n",
    "      # Uniform probability of select one of the paths\n",
    "      if accumulator >= r:\n",
    "          text.append(word)\n",
    "          break\n",
    "    if word in (None, '.') :      \n",
    "      sentences_generated = sentences_generated + 1\n",
    "  return \" \".join([word for word in text if word])\n",
    "\n",
    "# Calculate perplexity of a given sentence for a given model\n",
    "def perplexity(sentence, ngram_model):\n",
    "  ngram_model_n = len(list(ngram_model.keys())[0])\n",
    "  P = list()\n",
    "  N = len(sentence)\n",
    "  for i in range(ngram_model_n,len(sentence)):\n",
    "    if tuple(sentence[i-ngram_model_n:i]) in ngram_model and \\\n",
    "       sentence[i] in ngram_model[tuple(sentence[i-ngram_model_n:i])]:\n",
    "      P.append(ngram_model[tuple(sentence[i-ngram_model_n:i])][sentence[i]])\n",
    "    else:\n",
    "      return \"Cannot found this path\"\n",
    "  PP = np.power(np.reciprocal(np.prod(P)),1/N)\n",
    "  return PP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Qt98Tj4Cjxto"
   },
   "outputs": [],
   "source": [
    "# Don Quijote\n",
    "# with open(base_path+\"pg2000.txt\", \"r\", encoding=\"utf-8\") as myfile:\n",
    "#     text = super_cleaner(myfile.read())\n",
    "# text = text.replace(\"\\n\", \" \")\n",
    "# text = text.replace(\"[deleted]\", \"\")\n",
    "# sentences = split_sentences(text)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://www.kaggle.com/datasets/miguelcorraljr/ted-ultimate-dataset\n",
    "! wget -q --no-check-certificate -O 'ted_talks_es.csv' 'https://docs.google.com/uc?export=download&id=1C8QDQ9ecuSHvYeRQtB5LVfhBrfYIoBiZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1__HGcP8O1xr"
   },
   "outputs": [],
   "source": [
    "## Ted Talks\n",
    "import csv\n",
    "transcripts = []\n",
    "with open('ted_talks_es.csv') as csvfile:\n",
    "    ted_reader = csv.DictReader(csvfile)\n",
    "    sentences = [split_sentences(row['transcript']) for row in ted_reader]\n",
    "    sentences = list(chain(*sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "DsvNQI9CVuIa"
   },
   "outputs": [],
   "source": [
    "# text='hola esto es un ejemplo que lo escribo yo mismo. hola esto puede ser otro ejemplo o es un coche o es rojo. hola esto debe ser un ejemplo. Aunque no lo tengo tan claro, sabes? Qué te parece el resultado?'    \n",
    "# sentences = split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nSzOXbiYjqF4"
   },
   "outputs": [],
   "source": [
    "# text = \"Hugo tuvo un tubo, pero el tubo que tuvo se le rompió. Para recuperar el tubo que tuvo, tuvo que comprar un tubo igual al tubo que tuvo.\"\n",
    "# sentences = split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfDa75LwfUcT",
    "outputId": "d8bf19b1-6bef-4b1b-eafe-d70331d4766f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_train: 416746 s_test: 21935\n"
     ]
    }
   ],
   "source": [
    "# Split train test sentences and clean them\n",
    "s_train, s_test = train_test_split(sentences, test_size=0.05)\n",
    "print(f\"s_train: {len(s_train)} s_test: {len(s_test)}\")\n",
    "pool_obj = multiprocessing.Pool()\n",
    "test_sentences = pool_obj.map(text_cleaner,s_test)\n",
    "train_sentences = pool_obj.map(text_cleaner,s_train)\n",
    "\n",
    "# Join sentences as validation doesn't work properly\n",
    "train_sentences += test_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OEZs_JE0hRz1"
   },
   "outputs": [],
   "source": [
    "# train_sentences = train_sentences + test_sentences\n",
    "# Generate training words and flat the array\n",
    "train_words = list(map(methodcaller(\"split\", None), train_sentences))\n",
    "train_words = list(chain(*train_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZrNDviS8WrDZ"
   },
   "outputs": [],
   "source": [
    "# Generate models\n",
    "qgram_model = train_ngram_model(train_words, n =4)\n",
    "trigram_model = train_ngram_model(train_words,n=3)\n",
    "bigram_model = train_ngram_model(train_words,n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is too big\n",
      "Model is too big\n",
      "Model is too big\n"
     ]
    }
   ],
   "source": [
    "draw_graph(bigram_model, \"bigram\")\n",
    "draw_graph(trigram_model, \"trigram\")\n",
    "draw_graph(qgram_model, \"qgram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuvo un tubo comienza a su madre de neuronas en el hecho que observar en búsqueda indexada es una manera que la arquitectura es Özlem . Ahora les gusta . Y es Satanás .\n",
      "tuvo un tubo . Quizá deberíamos utilizar una sola nación . Y han llegado del todo seguro donde los fabricantes de base y se endurecen de forma estructurada y probar , procesar y exportaron productos de desecho que se censure .\n",
      "error: secuencia no encontrada\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentences(\"tuvo un tubo\".split(), bigram_model, 3))\n",
    "print(generate_sentences(\"tuvo un tubo\".split(), trigram_model, 3))\n",
    "print(generate_sentences(\"tuvo un tubo\".split(), qgram_model, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esto es un ejemplo , en direcciones opuestas que piensan en varias personas creaban herramientas es el menú del tiempo . Es la adversidad no buscan el que ocultar secretos fácilmente son los elefantes , y sin nunca se escape . Usamos energía , copias de las ruedas , días y asiático , retengan nuestros recursos , beautiful rechaza el caminar .\n",
      "esto es un ejemplo . La obra consta de dos mundos tridimensionales realmente se diferencian terminalmente . Basándonos en miles de personas cuyo trabajo es extraordinario no es la que enseñen esto a través de sus nuevos vehiculos serían flex - fuel compatibles , y estaba siendo aplastada , y en aumento en la arqueología nos brinda una gran decepción de esperanzas , de Star Wars .\n",
      "esto es un ejemplo característico de la humanidad , al menos , a mí sí . Sabemos que el cambio tecnológico en sí . Y dice , No .\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentences(\"esto es un ejemplo\".split(), bigram_model, 3))\n",
    "print(generate_sentences(\"esto es un ejemplo\".split(), trigram_model, 3))\n",
    "print(generate_sentences(\"esto es un ejemplo\".split(), qgram_model, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuUhFGUgarTq",
    "outputId": "e3150e02-65eb-4bbb-f117-49f392202fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me gustaría mucho más lo que yo me fue simplemente extendió como un mundo un balón de salud en mi rumbo . Y un videoblog con suerte se moverá un bizcocho de tecnología para esto por ahí para interactuar con estas 1 , a la pérdida de la vivienda , o deslegitimar las elecciones durante dos metros , quiere decir , supongamos que define como la incipiente .\n",
      "me gustaría mucho escuchar sus cuentos y también para nuestra economía , realmente sólo la mitad del escenario . Sus teléfonos se está llevando a cabo por cualquier acto de conmemoración camuflada en el norte de nuesto personal fué sigificativamente mayo que habría sido tratado con antibiótico es simplemente recordar que cada lado .\n",
      "me gustaría mucho que comencemos a tumbarlos y a olvidar cosas . Pero , ¿ quién ? El conocimiento no tiene que ocurrir .\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentences(\"me gustaría mucho\".split(), bigram_model, 2))\n",
    "print(generate_sentences(\"me gustaría mucho\".split(), trigram_model, 2))\n",
    "print(generate_sentences(\"me gustaría mucho\".split(), qgram_model, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFFHOqS3LdKf",
    "outputId": "4cab8ce8-1412-4dae-8515-90a4a8ec51d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porque somos parte posterior , especialmente cuando no es analizar una extremidad para llegar a capturarlo allí hace que tendré gente en encontrar jóvenes con Ud . Amigo , en general : Sí .\n",
      "Porque somos parte de esto es un gran rendimiento . La gente pobre no puede ser demostrado , contribuyen al bien público .\n",
      "Porque somos parte de la historia . Gracias .\n"
     ]
    }
   ],
   "source": [
    "print(generate_sentences(\"Porque somos parte\".split(), bigram_model, 2))\n",
    "print(generate_sentences(\"Porque somos parte\".split(), trigram_model, 2))\n",
    "print(generate_sentences(\"Porque somos parte\".split(), qgram_model, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7kvDpZn2qhrL",
    "outputId": "4479b0b1-a214-440e-b280-7fef027bf5c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El propósito y la alegría de la ciencia y la ingeniería .\n",
      "2.5685399630859127\n",
      "13.645760650251104\n",
      "44.702804712732366\n"
     ]
    }
   ],
   "source": [
    "ts = \"El propósito y la alegría de la ciencia y la ingeniería .\"\n",
    "print(ts)\n",
    "print(perplexity(ts.split(), qgram_model))\n",
    "print(perplexity(ts.split(), trigram_model))\n",
    "print(perplexity(ts.split(), bigram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHAfqOfJRnbT",
    "outputId": "9a199530-e5c7-47fa-b245-97c83214ce90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.898817599538185\n",
      "14.42645186106499\n",
      "101.67247517650911\n"
     ]
    }
   ],
   "source": [
    "ts = \"Y eso es importante no solo de su verdadera vida psicológica\"\n",
    "print(perplexity(ts.split(), qgram_model))\n",
    "print(perplexity(ts.split(), trigram_model))\n",
    "print(perplexity(ts.split(), bigram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLJkEPeXR620",
    "outputId": "1eb158f2-42d7-49cf-813f-03d86151960b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.335141362540313\n",
      "8.862856327032796\n",
      "76.05991219113524\n"
     ]
    }
   ],
   "source": [
    "ts = \"Mira , tengo estos nuevos submarinos nucleares que estoy construyendo\"\n",
    "print(perplexity(ts.split(), qgram_model))\n",
    "print(perplexity(ts.split(), trigram_model))\n",
    "print(perplexity(ts.split(), bigram_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hXh403_O-BeR"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "176155edaddee96d48759d15c0ad2ec7a961fab8ae45db547785485661763bbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
